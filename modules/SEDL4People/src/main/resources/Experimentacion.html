<html ><head>	<meta http-equiv="Content-Type" content="text/html;charset=ISO-8859-1"><meta charset="UTF-8">	<!--<link rel="stylesheet" href="https://exemplar.us.es/css/bootstrap.css">		<script type="text/javascript" src="https://exemplar.us.es/js/vendor/bootstrap.js"></script>	 Latest compiled and minified CSS -->	 	<script type="text/javascript" src="http://code.jquery.com/jquery-2.1.4.min.js"></script>		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap-theme.min.css">		<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script></head><body>	<h2>		 <a role="button" data-toggle="collapse" href="#intro" aria-expanded="false" aria-controls="intro">			EXPERIMENTATION	    		</a>	</h2>	<div id="intro" class="collapse" >		<p>This chapter provides the basic concepts on experimentation required to understand the contributions of this dissertation. First, Section 1 provides a definition of experiment and describes its life-cycle. Section 2 presents two sample experiments used throughout this and subsequent chapters. Next, Section 3 introduces the basic concepts related to description of experiments including the variables, hypotheses and design. Section 4 describes the concepts and techniques used during the execution of the experiments and the statistical analysis of their results. The concept of experimental validity is presented in Section 5. Section 6 describes the specific issues of MOEs, and the usual types of experiments performed in the context of the MPS life-cycle. Additionally, sections 6.3 and 6.4 describe respectively some specific experimental designs and analyses proposed in literature for MOEs. </p>			<h3>			<a role="button" data-toggle="collapse" href="#experimentConcept" aria-expanded="false" aria-controls="experimentConcept">				1 THE CONCEPT OF EXPERIMENT	    			</a>		</h3>	<div id="experimentConcept" class="collapse">			<p><i>Experimentation</i> refers to a methodical procedure of actions and observations with the goal of empirically verifying or falsifying a hypothesis. Shadish et al. define experiment as <i>"A study in which an intervention is deliberately introduced to observe its effect". For instance, in the context of a medical experiment with a new antipyretic (drug used to reduce the fever), the intervention would be to treat feverish patients with the drugs, and the observation is to measure the reduction of body temperature along time.</i> </p>			<p>Related concepts are <i>natural experiments</i> and correlational studies. Natural experiments are studies where the cause usually cannot be manipulated i.e., there is not intervention, and the study contrasts a naturally occurring event such as the temperature of the ocean. Correlational studies (a.k.a. non-experimental or observational studies) simply observe the size and direction of a relationship among variables without establishing a relationship of causality or generalizing the relationship out of the observed data. Natural experiments and correlational studies are not experiments according to the definition provided by Shadish et al. </p>			<p>In this dissertation the concept of experiment is more general including both natural experiments and correlational studies, namely <i>a process of systematic inquiry and data collection with the aim to confirm or disprove a hypothesis.</i> </p>			<p>Thus, an experiment is not a static object, it is a process that flows through an ordered sequence of activities, from the formulation of the experimental hypothesis to the drawn of conclusions regarding the hypothesis. Figure 1 depicts this process as a life-cycle. </p>			<img  src="imagenes/lifeCycle.jpeg" alt="Experimental life-cycle" style="  display: block;    margin-left: auto;    margin-right: auto; max-width: 600px;">			<p align="middle">Figure 1: Experimental life-cycle<p>			<p>The first activity in the experimentation life-cycle is the statement of the experimental hypothesis. This activity comprises of several steps: the identification of the research problem or question, a survey of the literature to check if the question has been answered by others, and the reduction of the research question to a testable hypothesis. For instance this reduction will transform the question "is the new antipyretic drug effective?" to a hypothesis such as "the average reduction of corporal temperature in feverish patients, measured after 2 hour of the administration of a dose of 100mg of the drug, is statistically significant". </p>			<p>The design of the experiment is the next activity. It comprises of: the selection a sample for the experiment, the identification of the instruments and artefacts required for experimentation, and the creation of plan for data collection and analysis. The output of this activity is thus a detailed plan-called experimental protocolthat tries to maximize the information obtained from the experiment. </p>			<p>Next, experimental artefacts are developed. In social sciences this activity usually involves the creation of the forms used to collect information, and a database or some computer support for storaging the information. In computational experiments this activity also involves the implementation of the algorithms. Once all experimental artefacts are available, the experiment is conducted and data is collected. Next the data is analysed, usually through inferential statistics. Finally, based on the results of such analysis, either conclusions are drawn or the experimental hypothesis and design are modified leading to a new execution of the life-cycle to reach new conclusions. </p>		</div>		<h3>				<a role="button" data-toggle="collapse" href="#samplex" aria-expanded="false" aria-controls="samplex">				2 SAMPLE EXPERIMENTS 			</a>		</h3>		<div class="collapse" id="samplex">			<p>Along this chapter two basic experimentation scenarios are used to illustrate the basic concepts about experimentation. The first one is taken from the medical area, and the other is taken from the specific area of application of this thesis -optimization problem solving with metaheuristics-. This selection is intentional since our aim is to show that experimental concepts presented in this chapter are general and potentially applicable to any area of human knowledge. </p>			<p><strong>Experiment #1.</strong>The goal of this experiment is to discern if a new drug can be used as antipyretic and in which doses. A set of individuals with fever will be used in this experiment to evaluate the effects of the drug. The reduction of body temperature of patients will be measured, and the reduction induced by the new drug will be compared with the reduction generated by previous alternatives and placebos. </p>			<p><strong>Experiment #2.</strong> The goal of this experiment is to compare the effectiveness of several metaheuristic techniques in solving optimization problems in quality-driven web service composition2. In particular, the metaheuristics to be compared are Tabu Search (TS), Simulated Annealing (SA), Evolutionary Algorithms (EA), Greedy Randomized Adaptive Search Procedure (GRASP) and GRASP hybridized with Path Relinking (GRASP+PR). For the experiment, each metaheuristic will be implemented in an specific metaheuristic program. Then, each program will be run to find solutions for a set of given optimization problems comparing their results. </p>		</div>		<h3>				<a role="button" data-toggle="collapse" href="#expdesc" aria-expanded="false" aria-controls="expdesc">				3 EXPERIMENTAL DESCRIPTION 			</a>		</h3>		<div class="collapse" id="expdesc">			<p>In this section, we focus on the concepts regarding the initial activities of the experimental life-cycle. This mainly refer to the selection of the experimental objects and subjects, hypothesis statement and experimental design. Figure 2 depicts the concepts described in this section and its relationships as a conceptual map. </p>			<img src="imagenes/ConceptualMap.jpg" style="  display: block;    margin-left: auto;    margin-right: auto; max-width: 600px;">			<p align="middle">Figure 2: Conceptual map about experimental description </p>			<h4>				<a role="button" data-toggle="collapse" href="#objects31" aria-expanded="false" aria-controls="objects31">					3.1 Objects, subjects and populations				</a>			</h4>			<div class="collapse" id="objects31">				<p>The first element to be defined in any experiment are the experimental objects (a.k.a. experimental units). The experimental objects are the elements of interest that participate in a particular experiment. In the social and biological sciences, those objects are usually people, and are called participants or individuals. For instance in experiment #1 experimental objects are sick people with fever. In experiment #2, the experimental objects represent algorithm runs. </p>				<p>The target population of an experiment is defined as the set of experimental objects to which we would like to generalize the conclusions obtained from the experiment, i.e., the subject of the hypothesis of the experiment. The target population of experiment #1 could be all adult humans with fever. The target population of experiment #2 could be all the runs of an algorithm on any instance of the optimization problem to solve. Since the target population of an experiment can be huge or even infinite, the researcher usually uses a sample of it in the experiment. The sampling of the target population can be divided in two phases. First the accessible population (a.k.a. sampling frame) is defined. The accessible population is the set of experimental individuals that could participate in the experiment. For instance, the accessible population of experiment #1 could be the set of patients diagnosed with fever in an specific hospital in the province of Seville. The selected sample is the set of experimental objects that actually participate in the experiment, taken from the accessible population. This selection can be performed in a number of different ways, from random to convenience. For instance, the selected sample of experiment #1 could be 20 individuals with fever chosen at random from the Seville Hospital. </p>				<p>Experimental subjects (a.k.a. experimenters) are the people who apply the methods, techniques or treatments to the experimental objects. For instance, in experiment #1, the specific doctors or nurses that administer the drug to the patients are the experimental subjects. In some cases, experimental subjects may influence the results of the experiments and therefore they must be controlled during the design of the experiment, i.e., experimental subject are treated as a variable. </p>				<img src="imagenes/ConceptualMap.jpg" style="  display: block;    margin-left: auto;    margin-right: auto; max-width: 600px;">				<br>				<p align="middle">Figure 3: Experimental objects, populations and sample</p>			</div>			<h4>				<a role="button" data-toggle="collapse" href="#variables32" aria-expanded="false" aria-controls="variables32">					3.2 Variables				</a>			</h4>			<div class="collapse" id="variables32">				<p>An experimental variable is defined as a characteristic of the experimental objects or of the experimental environment that can have different values. For instance, in experiment #2 the specific metaheuristic technique applied for solving a problem in a run is a variable, since it varies between runs. When a relevant characteristic has only one value in the context of an experiment, it is a constant (a.k.a. parameter). For instance, in experiment #2 the termination criterion is the same for all the metaheuristics in the experiment, thus it is a constant. </p>				<img src="imagenes/Variable.jpg" style="  display: block;    margin-left: auto;    margin-right: auto; max-width: 600px;">				<br>				<p align="middle">Figure 4: Taxonomy of experimental variables</p>				<p>The roles that a variable can play in an experiment are outcome (a.k.a. output, response or dependent variable), and factor (a.k.a. independent variable or input). This taxonomy is depicted in Figure 4 as an UML class hierarchy. </p>				<p>The outcome is the presumed result of the experiment, and its value is used for testing the hypothesis of the experiment. It measures or assesses the effect of factor(s). For instance, in experiment #2 the value of the objective function for the best solution found by each metaheuristic is the outcome. The outcome of experiment #1 is the difference of body temperature after two hours of dose administration.<p>				<p>Factors can be classify into two types, controllable factors and non-controllable factors. A controllable factor (a.k.a. active independent variable) is a variable whose value is applied or given to the experimental objects. The values of this kind of variable are usually controlled or manipulated in some way by the experimenter. A controllable factor in experiment #1 is the dose administered to the patients. A controllable factor in experiment #2 is the specific optimization technique applied in a run. The values of a non-controllable factor are not changed during the study. For instance, the age and the gender of patients in experiment #1 are non-controllable factors since they are not modified by experimenters but are different among patients.</p>				<p>The different values of a factor could make the outcomes of the experiment not comparable. When this risk exists, the experimental objects must be grouped into blocks according to the value of the factor, which is used as a so-called blocking variable. This creates homogeneous blocks that receive the same treatment along the experiment making the results comparable. For instance, in experiment #1, the effects of the drug measured in children and adults could not be comparable. Thus, the age of the patients could be used as a blocking variable dividing patients into two blocks, those under 18and those aged 18 and older. </p>				<p>If a factor is of no particular interest in an experiment, but could be useful in subsequent replications or its impact on the response is unknown, it is called a nuisance variable (a.k.a. extraneous variable). Nuisance variables must be ruled out or controlled in order to ensure the validity of the experiment. A way to to control the effect of this kind of variables on the conclusions of the experiments is random assignment of experimental objects to experimental groups (this type of assignment procedure is described below). For instance, in experiment #1 the sex of patients is not interesting, but its impact on the effectiveness of antipyretics is unknown. Consequently, the specific drug and dose administered to patients should be chosen randomly. Randomization ensures that the effect of the sex factor is averaged among the results of all the drugs and doses, reducing the bias introduced in the results.</p>				<p>The levels of a variable are the set of values that it can have in the context of the experiment. For instance, in experiment #1, the levels of the "dose" variable could be: 0mg, 100mg, 200mg, etc. In experiment #2, the factor metaheuristic has five levels: EA, TS, SA, GRASP and GRASP+PR. </p>				<p>One important characteristic about variables is whether the levels are unordered categories or they are ordered from low to high3. For instance, the levels of the "metaheuristic" variable in experiment #2 are not ordered, since they are essentially labels. The variables whose levels are not ordered are said to be nominal variables. Conversely, ordered variables have a set values that vary from low to high within a certain range. Depending on the measurement scale of the levels, ordered variables can be divided into ordinal and real variables. In ordinal variables, the levels are ordered from low to high in a ranking, but the intervals between the various ranks are not equal. For instance, in a F1 race the second place car may finish twenty seconds after the winner but only a fraction of second before the third place car. Real variables (a.k.a. scalar variables) not only have levels that are ordered, but also the values associated to those levels are equally spaced. These variables are named rational or intervalar depending on whether they have a real zero value or not . It is worth noting that the kind of variable may not be directly related to the output of the mechanism used to measure it or the nature and range of its value in the real world. In this sense, we can have an ordinal variable, whose levels are "low" and "high", but whose values are measured as real values ranging from 0 to 50 (we must define the two intervals [0, X] and (X,50] that determine when a real value is high or low). In the same way, we can have two variables that are both rational, but whose levels have integer and floating point values respectively. The taxonomy of variables according to their levels is depicted in Figure 5 as an UML class hierarchy.</p>				<img src="imagenes/VariableLevels.jpg" style="  display: block;    margin-left: auto;    margin-right: auto; max-width: 600px;">				<br>				<p align="middle">Figure 3.5: Taxonomy of experimental variables according to their levels</p>				<p>The experimental objects that have the same levels for factors are usually arranged into groups. In this sense, the group denotes a specific set of individuals with specific experimental conditions along the conduction process. The mechanism used to decide the experimental individuals that pertain to each group, i.e., which treatment will be applied to them, is denoted as assignment. </p>			</div>			<h4>				<a role="button" data-toggle="collapse" href="#hypothesis33" aria-expanded="false" aria-controls="hypothesis33">					3.3 Hypotheses				</a>			</h4>			<div class="collapse" id="hypothesis33">							<p>The hypothesis of an experiment is a statement about its variables. This hypothesis can be related to an underlying theory that predicts that the hypothesis holds. The objective of an experiment is to disprove or confirm the hypothesis. According to Popper's concept of scientific truth, the theories whose predictions have not been disproved by experiments and for which no other alternative theory is available, are considered as true. Scientific hypothesis are divided into three types: difference (or differential), associational and descriptive.<p>				<h5>Differential hypotheses</h5>									<p>Differential hypotheses state that there is a difference in the value of the outcome of two or more groups, that is, the factors make a significant difference on the outcome. For instance, a differential hypothesis for experiment #2 is that the specific metaheuristic used for optimization (controllable factor) has a significant impact on the value of the objective function (outcome). Another typical differential hypothesis used in medicine is that a specific drug has an impact on the symptoms of a particular disease, 				e.g. whether the drug actually reduces the fever or not in experiment #1.</p>				<h5>Associational hypotheses </h5>								<p>Associational hypotheses state that there is a specific relationship between the levels of two variables. For instance, a typical associational hypothesis is that of a linear dependence in the value of two variables x and y, where y = a  x + b. In a possible associational hypothesis for experiment #1 would be that "the decrease in body temperature is proportional to the dose". </p>				<h5>Descriptive hypotheses </h5>								<p>Descriptive hypotheses state that the variables have some value, central tendency or variability, and summarize the data obtained. In this sense, descriptive hypotheses usually do not have factors, and their assertions refer only to the current sample; i.e., the target population, the accessible population and the sample are the same. A sample descriptive hypothesis for experiment #1 is that the reduction of body temperature for the specific individuals that participate in the experiment treated with the antipyretic range between 0.4 and 2.6 degrees. This kind of hypotheses allows including as experiments correlational studies and natural observations, which are usually performed to explore new questions, or are the only data available at the moment for one subject. </p>			</div>			<h4>				<a role="button" data-toggle="collapse" href="#design34" aria-expanded="false" aria-controls="design34">					3.4 Design				</a>			</h4>			<div class="collapse" id="design34">				<p>Experimental design is what differentiates scientific and engineering experiments from a careful natural observation. The main point of experimental design is controlling factors. Montgomery defines experimental design as "the process of planning the experiment so that appropriate data can be collected and analysed by statistical methods, resulting in valid an objective conclusions (regarding the experimental hypothesis)". We further refine this definition of Montgomery based on the description by Hinkelmann and Kempthorne as follows: Experimental design is the specification of (i) the actuations to be performed during experimental conduction -regarding the levels of the variables involved-, (ii) the specific experimental objects on which they will be performed, and 				(iii) the arrangement of (i) and (ii) with the aim to minimize experimental error and systematic bias. Thus, an experimental design should specify:</p> 								<ul>					<li><p>Selection of experimental objects. This is about selecting, from the accessible population, the experimental objects that will take part in the experiment. This is usually done by means of the algorithm used to perform the selection, not by enumerating the experimental objects explicitly. This algorithm is called the selection method. For instance, in experiment #1 the selection could be performed by choosing randomly 50 individuals among all the feverish patients in the accessible population. </p></li>					<li><p>Specification of variables and levels. This determines the factor variables that will be modified by experimental subjects during the experimental conductions and which levels will be set. The modification of the level of a factor variable is usually referred to as a treatment. For instance in experiment #1 the action associated with treatments are the administration of the drug, and the levels of the corresponding factor "dose" could be 0mg i.e., a placebo, 100mg, and 200mg. </p></li>				 						<li><p>Specification of treatments and groups. It specifies the experimental objects that will receive each specific treatment. The set of experimental objects that receive the same sequence of treatments are usually denoted as a group. This specification is also performed by means of the algorithm used to perform the assignment of experimental objects to groups, not by enumerating the experimental objects explicitly. This algorithm is usually named the assignment method. For instance, in experiment #1, the assignment method used to decide which treatments are applied to each one of the 50 individuals in the sample could be random. </p></li>				 						<li><p>Treatment and Measurement sequence. The specific sequence in which the experimental objects receive the treatments and the outcome variables are measured. Those details are finally expressed in an experimental protocol. </p></li>				</ul>				<p>It is worth noting that the experimental design determines the information that will be gathered from the experiment and the capability of the subsequent analyses to disprove or confirm the hypothesis. Thus, the consistency of the experimental design with regards to the experimental hypothesis is a crucial characteristic of any experiment. Experimental design is also intimately related to the internal validity of the experiment, since the specific arrangement of the treatments and measurements, and the methods used for selection and assignment of experimental objects can avoid most threats to validity. 				</div>				<h5>											Principles of experimental design									</h5>				<p>In order to assure validity of the analysis, and to increase its capability for providing clear conclusions regarding the hypothesis, experimental designs should fulfil three basic principles: </p>				<ul>				<p>1. 					Repetition. This principle establishes that each treatment must be repeated on different experimental objects a number of times. The pursued goal is reducing the bias introduced by the specific characteristics of every single experimental objects in the observations of the outcome variable. For instance, in experiment #1, the effect of each specific dose and antipyretic drug should be measured on several patients. Regarding how to determine the right number of repetitions, some information about the treatments, experimental objects and distribution of the outcome is need to determine how many repetitions to run. This information is known only to experimenters who are experienced in the experimental domain. If such information is not available, one option is to use values accepted in the literature of the domain. For instance, for simple comparison experiments with normal distribution of the outcomes, a sample size of 30 experimental objects with a single treatment and measurement per object is widely accepted as a reasonable minimum. </p>				<p>2. 					Randomization. This principle establishes that the decision on which treatment should be applied to each experimental object must be made according to a random scheme. The goal pursued is to reduce the bias introduce when all the repetitions of a treatment are performed on individuals with similar characteristics. For instance, in experiment #1 if the new drug would be administered to the youngest patients only, the results of the study could be biased since they are more sensitive to antypiretics. </p>				<p>3. 					Local Control or Blocking. The basic idea behind this principle is that when there are factors that make the outcomes of the experiment non comparable, the selected sample should be partitioned into blocks as homogeneous as possible. Within those blocks observations are comparable. The groups, treatments and observations are then replicated for each block. For instance, in experiment #2 we should have one block per problem instance, since executions of algorithms on different problem instances are not comparable. </p>				</ul>				<p>Next, some classical experimental designs compliant with the principles are presented. </p>								<h4>					<a role="button" data-toggle="collapse" href="#crdes" aria-expanded="false" aria-controls="crdes">						Complete randomized design					</a>				</h4>				<div class="collapse" id="crdes">					<p>The simplest design for differential hypotheses is the completely randomized design. Given t treatments and N = tr homogeneous experimental objects. The N experimental objects are partitioned in t experimental groups of r elements, each group with a different treatment, and the experimental subjects are assigned randomly to an experimental group. The outcome is measured once the treatment is performed on each experimental object. Thus the dataset generated contains N values for the outcome in total, and r observations for each specific treatment. This design requires that the assignment procedure of the experimental objects to groups and the specific treatment applied to each group is random, and that there are no blocking variables. </p>					<p>For instance, let's suppose that a single problem instance is going to be optimized in experiment #2. In those circumstances a complete randomized design would be appropriate. We could set the number of experimental objects (algorithm runs) to 100 with one group for each level of the factor "metaheuristic" (TS. SA, EA, GRASP and GRASP+PR). Each group would comprise of 20 experimental objects (algorithm runs). The specific order of execution of the runs would be chosen randomly. </p>				</div>				<h4>					<a role="button" data-toggle="collapse" href="#rcbdes" aria-expanded="false" aria-controls="rcbdes">						Randomized complete block design					</a>				</h4>				<div class="collapse" id="rcbdes">				<p>There are many situations with systematic variation among sets of experimental objects depending on its factors. In such situations, a random design is not feasible since the design should take this variation into account to "eliminate" its effect on the conclusions making observations comparable. This leads us to the concept of local control or blocking, introduced by Fisher. For instance, in experiment #2 the variable "instance" has an strong impact on the value of the outcome "objectiveFunction". Consequently, the results obtained by techniques for different problem instances cannot be compared. Randomized complete block designs are used for experiments with differential hypotheses and a single blocking variable. In a randomized complete block design, the selected sample is divided into b sets of homogeneous experimental objects called blocks. A complete randomized design is performed on each block. Thus the dataset generated contains b  N values for the outcome variable in total, and b  r observations for each specific treatment. </p>				<p>For instance, in experiment #2 a randomized complete block design could specify a number of objects (algorithm runs) equal to 500. There will be 5 groups with their corresponding levels of the factor variable "metaheuristic". There will be 10 blocks, one per level of the blocking variable "instance". Each group (and the corresponding algorithm) will comprise of 10 experimental objects (runs) chosen randomly. Thus, the dataset generated contains 500 values for outcome variable "objectiveFunction" in total, and 100 values for each specific optimization technique, and 50 values for each specific problem instance. </p>				</div>				<h4>					<a role="button" data-toggle="collapse" href="#latsqu" aria-expanded="false" aria-controls="latsqu">						Latin square					</a>				</h4>				<div class="collapse" id="latsqu">					<p>The Latin square design is used for experiments with differential hypotheses and two blocking variables. It is used to compare t different treatments in a matrix with t rows and t columns. The rows and columns actually represent two restrictions on randomization. In this design a single treatment is used for each combination of levels of each blocking variable. This means that the treatments of the elements for each row and column of the matrix are different (no treatment is repeated per row and column). </p>					<p>For instance, in experiment #2 the techniques available to find solutions for the optimization problem are TS, SA and EA. The experiment has a single factor, i.e., the technique used to solve the problem. It also has two blocking variables, the specific problem instance (I1, I2, and I3) and the termination criterion used: 100, 5000, 10000. Two latin squares for experiment #2 are shown in table 1. </p>					<img src="imagenes/LatinSquares.png" style="  display: block;    margin-left: auto;    margin-right: auto; max-width: 400px;">					<br>					<p align="middle">Table 1 - Two Sample 3x3 latin squares for a technique comparison experiment</p>					<p>It is worth noting that latin squares are reduced (or incomplete) designs, in the sense that not every treatment is applied under every combination of levels of the blocking variables. Thus, its experimental protocol requires less treatments and measurements (optimization techniques runs in this example) than randomized complete block designs, making experimental conduction faster and cheaper. In our example, the latin square design requires 9 runs while the randomized complete block design requires 27 runs (assuming that no repetition is performed, i.e., groups size is 1). </p>				</div>				<h4>					<a role="button" data-toggle="collapse" href="#factdes" aria-expanded="false" aria-controls="factdes">						Factorial designs					</a>				</h4>				<div class="collapse" id="factdes">					<p>Factorial designs are used when several factors are present in the experiment. For instance, in experiment #1 there are two controllable factors drug and dose. The factor drug could be nominal, with hypothetical levels "never-fever", "colder-plus", and "bye-fever". The factor dose could be scalar with levels 10mg, 50mg,. . . In this case, a treatment consists of a combination of levels for drug and dose. The experimental protocol of factorial designs varies the levels of each of controllable factor until considering all possible level combinations simultaneously. Factorial experiments are used widely in scientific and industrial experimentation because they allow to evaluate the effects each factor and their combinations (named interactions). A 2k factorial design is a specific kind of factorial design where k factors with 2 levels are studied. </p>					<p>For instance, let us consider a slight modification of experiment #2. Instead of comparing different techniques we could compare different variants of EA. In particular, let suppose we compare two alternative crossover operators (uniform and one-point) and two alternative selection strategies (roulette and tournament). In this situation, with two controllable factors, a factorial design is appropriate. Specifically we have a 22 factorial design describe the 4 possible tailorings of the EA. The experimental protocol would have 4 groups of equal size, i.e., one per possible combination of the levels of the factors. The sequence of application of the treatments would be random. </p>				</div>		</div>		<h3>			<a role="button" data-toggle="collapse" href="#expexec" aria-expanded="false" aria-controls="expexec">				4 EXPERIMENTAL EXECUTION			</a>		</h3>		<div class="collapse" id="expexec">			<p>In this section, we focus on the concepts regarding the final activities of the experimental life-cycle, denoted as experimental execution. According to our definition of the experimentation life-cycle, experimental execution comprises of two phases, experimental conduction and data analysis. </p>			<p>The description of an experiment is not enough for automating the replication of the experiment. Even for experiments in computer science where experimental protocols are implemented as programs (such as metaheuristics optimization experiments), more details are required to perform an automated conduction of the experiment. In order to support such automation, a detailed description of the process of experimental conduction, including its inputs and outputs is required. Additionally, in order to evaluate if an experiment could be replicated, additional details are required (such as ranges for environmental and extraneous variables, human resources required for replication, material equipment, etc.). Experimental conduction involves treatment application and data collection. This activity should be performed in an unbiased and objective way. </p>			<p>Analysis of data is the process of inspecting and modelling data with the goal of discovering information, draw conclusions, and support decision making. Statistics is the basic tool used to draw conclusions from the data retrieved during the data analysis phase. The specific type of statistical procedure to be used for data analysis depends strongly on both the type of hypothesis and the design of the experiment, and it is usually performed in two phases: exploratory and confirmatory analyses. </p>			<p>Throughout exploratory data analysis is possible to detect mistakes in experimental conduction, check the assumptions taken during experimental design, and assess the direction and rough size of the relationship among the experimental variables. In turn, throughout confirmatory data analysis is possible to confirm or disprove the hypothesis of the experiment by means of statistical inferences. </p>						<h4>				<a role="button" data-toggle="collapse" href="#expdata" aria-expanded="false" aria-controls="expdata">					4.1 Exploratory data analysis				</a>			</h4>			<div class="collapse" id="expdata">				<p>There is a plethora of disparate techniques that are used for exploratory data analysis that can be classified in very different ways. A possible manner divides such techniques into three groups: tabulations of the data, graphs (a.k.a. charts) and descriptive statistics. Graphs are visual representations of the sample, being histograms and bar charts two of the most widely used graph type. However, there exists a wide graph type offer, which is very useful for more specific purposes. For example, the so√êcalled scatter-plot is often used in case of experiments with relational hypotheses since it allows observing the degree of association between two variables. </p>								<p>Descriptive statistics can be further sub-classified into central tendency measures and variability measures. Central tendency measures such as the mean, the median and the mode describe the way the values of a sample cluster around some value. The mean (a.k.a. arithmetic average) is appropriated when the sample follows a normal distribution in absence of outliers, and it is defined only for real variables. The median (a.k.a. middle score) is more suitable than the mean when the frequency distribution is skewed markedly or outliers are present, and it is defined for both real an ordinal variables. Finally, the mode, the most common value in the sample, is usually the least precise central measure for real and ordinal variables, but it can be computed for nominal variables. </p>				<p>Variability measures describe the spread or dispersion of a given sample. Thus, in an extreme case where all the scores in a distribution are the same, the variability of the sample is zero. Standard deviation and inter-quartile range (distance between the 25th and 75th percentiles) are the most commonly used variability measures for real and ordinal variables respectively. For nominal variables, an usual variability measure is the number of different categories present in the data and the percentage of the samples in each category. </p>			</div>			<h4>				<a role="button" data-toggle="collapse" href="#confdata" aria-expanded="false" aria-controls="confdata">					4.2 Confirmatory data analysis				</a>			</h4>			<div class="collapse" id="confdata">				<h5>					<a role="button" data-toggle="collapse" href="#overview" aria-expanded="false" aria-controls="overview">						Overview					</a>				</h5>				<div class="collapse" id="overview">					<p>Using the appropriate confirmatory data analysis technique is crucial to obtain true conclusions for a specific experimental design and hypothesis. Table 2 shows the relationship among hypothesis types, number of factors, and appropriate type of statistical technique used for confirmatory data analysis. Tables 3, 4, and 5 show the specific statistical techniques to be used under specific circumstances. It is worth noting than although tables 3, 4, and 5 provide a number of statistical techniques and a complex selection framework, the casuistic and set of techniques to be used is still incomplete. For instance, in the cases of the application of ANOVA or Friedman tests, additional post-hoc procedures such as Bonferroni, Holm, etc., are usually required to decide if a specific treatment is better than other. This complexity is one of the main motivations for our goal of automating this task of the experimental life-cycle. </p>										<img src="imagenes/Statistical.png" style="  display: block;    margin-left: auto;    margin-right: auto; max-width: 600px;">					<br>					<p align="middle">Table 2: Statistical procedure decision table.</p>					<img src="imagenes/SpecificSTH.png" style="  display: block;    margin-left: auto;    margin-right: auto; max-width: 600px;">					<br>					<p align="middle">Table 3: Specific STH for basic experiments with a single independent variable</p>					<img src="imagenes/MultipleIndependentVariables.png" style="  display: block;    margin-left: auto;    margin-right: auto; max-width: 600px;">					<br>					<p align="middle">Table 4: Specific STH for experiments with multiple independent variables</p>										<img src="imagenes/RegressionCoefficients.png" style="  display: block;    margin-left: auto;    margin-right: auto; max-width: 600px;">					<br>					<p align="middle">Table 5: Regression coefficients and models</p>					<p>					According to table 2, the recommended technique for confirmatory data analysis depends strongly on the type of experimental hypothesis. For instance, for associational hypotheses with a single factor, correlation coefficients such as the Pearson product moment correlation and the Spearman rho could be used. Providing a comprehensive description of each method described in tables 3,4, and 5 is out of the scope of this dissertation.</p>					<p>Since the types of MOEs taken into account in this dissertation use differential hypotheses the primary analysis mechanism used in practice is Statistical Testing of Hypotheses5 (STH). STH is applied to decide whether there are significant differences between datasets of experimental results. Consequently, can assess if a metaheuristic, tailoring or tuning is better than other. STH is described in deeper detail in the next subsection. </p>				</div>				<h5>					<a role="button" data-toggle="collapse" href="#statistical" aria-expanded="false" aria-controls="statistical">						Statistical Testing of Hypotheses					</a>				</h5>				<div class="collapse" id="statistical">					<p>STH work by defining two hypotheses, the null hypothesis H0 and the alternative hypothesis H<sub>1</sub>. The null hypothesis is a statement of no effect or no difference, whereas the alternative hypothesis represents the presence of an effect or a difference. 					5Confidence intervals are also used, but STH is more popular </p>					<img src="imagenes/AcceptanceTest.png" style="  display: block;    margin-left: auto;    margin-right: auto; max-width: 600px;">					<p align="middle">Figure 3.6: Hypothesis acceptance and rejection areas</p>					<p>					Thus, if H<sub>1</sub> holds, then significant differences exist between groups of individuals, the performance of algorithms, or the effect of a technique or methodology for software development. Both hypothesis are mutually exclusive; i.e., if H<sub>0</sub> holds then H<sub>1</sub> does not hold, and vice-versa 					The result generated by most statistical tests is a p-value. A p-value is the probability of the observations provided as result of the experiment assuming that H<sub>0</sub> is true. A p-value provides information about whether a hypothesis test is significant or not, and it also indicates something about how significant the result is: the smaller the p-value, the stronger the evidence against H<sub>0</sub>. Decision making on the hypotheses using statistical tests is performed by imposing a minimum threshold for the p-value from which we consider that the null hypothesis H<sub>0</sub> is false. This threshold is named the significance level and denoted as a. Figure 6 shows a diagram that describes the roles of a and the p-value in the decision making about the hypotheses. The usual process for applying STH is: </p>					<p>					1. 						<i>Map the experimental hypothesis of the experiment into a pair of statistical hypotheses (H<sub>0</sub> and H<sub>1</sub>)</i>. The hypotheses are described in terms of the parameters of the distributions of random variables from which a sample can be obtained by conducting the experiment (the data-set to be analysed). As a consequence, researchers must identify metrics that measure the variables that appear in the experimental hypothesis, and define the mechanism that will be used to instrument them. For instance, in MOEs the performance of a metaheuristic is usually measured by the value of the objective function for the best solution found in a run with a specific termination criterion. Once the random variables are identified and its instrumentation mechanisms are specified, statistical hypotheses H0 and H1 are stated. The mapping between the experimental and the statistical hypothesis is usually performed in terms of the parameters of the distributions of the random variables. For instance in experiment #1 the null hypothesis H0 would state that "there is no significant difference on the means of the distribution of values for 					the objective function provided by the different techniques; i.e., that their performance is similar." </p>					<p>					2. 						<i>Decide which statistical analysis procedure is appropriate</i> (the specific test of hypothesis). The main factors affecting this decision are: the type of hypothesis, the experimental design, and the nature of the data. Furthermore, this last factor is in turn twofold: the type of the variables can be nominal (e.g. X, Y Z), ordinal 					(e.g. good, fair, bad) or intervalar/ratio (e.g. 10.0, 5.0, 2.0), and the statistical properties of the data-set. In case of non-gaussian distributions non-parametric tests should be used. </p>					<p>					3. 						<i>Select a significance level (a)</i>. It is widely accepted that if the p-value is lower than 0.05, there is enough evidence to reject the null hypothesis H0 and assume that H1 holds. </p>					<p>					4. 						<i>Compute the p-value.</i> In order to compute the p-value, the value of the test statistic T must be previously calculated from the dataset. Given the test statistic T and the expected distribution of the data D, the p-value is computed. </p>					<p>5. 						<i>Decide to reject null hypothesis.</i> It is widely accepted that if and only if the p-value is less than the significance level a the null hypothesis must be accepted. For instance, in the example shown in Figure 6 the value of the p-value is not below the selected a, thus there is no evidence for rejecting the H0. </p>					<p>When STH is used to detect significant differences among two distributions (the null hypothesis would be that the distributions are identical), they are called simple comparison tests. Conversely, when STH is used to detect significant differences among three or more distributions (the null hypothesis would be that all the distributions are identical), they are called multiple comparison tests. The use of such a null hypothesis in multiple comparison tests involves that the alternative hypothesis states that there are at least one distribution that is different from the rest. If the null hypothesis in a multiple comparison test is rejected, then we know that are significant differences but ignore among which of the distributions. Thus, in order to reach to concrete conclusions about which specific distributions are different form others, and additional type of statistical technique named post-hoc procedure must be applied. </p>					<p>Post-hoc procedures are a special kind of STHs, concerned with finding relationships among a couple of distributions from the associated multiple comparison test. They control the accumulation of potential errors that derives for linking a sequence of statistical tests in order to provide a global significance level for all the comparisons performed. For each specific of multiple comparison test (such as ANOVA or Friedman test), a specific set of post-hoc procedures has been defined in the literature. </p>				</div>			</div>		</div>		<h3>			<a role="button" data-toggle="collapse" href="#experimentalVal" aria-expanded="false" aria-controls="experimentalVal">				5 EXPERIMENTAL VALIDITY 			</a>		</h3>		<div class="collapse" id="experimentalVal">			<p>The term <i>validity</i> refers to the approximate truth of a statement or inference. In the context of experimentation, it is usually applied to the conclusions regarding the hypothesis of an experiment. Thus, when an inference about the hypothesis (whether that it holds or not) is valid, it means that there is sufficient evidence, both in the data and the experimental process, to support the inference. Validity is thus a property of inferences (or conclusions), it is not a property of designs or hypotheses. For example, using a complete randomized design does not guarantee the validity of an inference about the effectiveness of the antipyretic in experiment #1. There could be many reasons invalidating the inference. For instance, in experiment #1 the nurses could administer an erroneous dosis of the drug leading to wrong conclusions. For the sake of simplicity and in accordance with most of the literature on the topic, we will define an experiment as valid if it allows to draw valid conclusions. </p>			<p>Threats to validity are the specific causes the compromise the validity of a conclusion. In this dissertation the enumeration of threats to validity presented by Shadish et al. is used with slight adaptations to our terminology. In general terms, threats to validity can be divided into two groups: internal and external threats. The formers are those that affect to the validity of the conclusions of the experiment. The later are those that put at risk the generalization of the conclusions obtained. In the next subsections we list some of these threats. </p>					<h4>				<a role="button" data-toggle="collapse" href="#internalVal" aria-expanded="false" aria-controls="internalVal">					5.1 Internal validity 				</a>			</h4>			<div class="collapse" id="internalVal">				<p>The <i>internal validity </i> of an experiment is defined as the extent to which we can infer that the hypothesis holds (or not) from the experimental process and data gathered. Consequently, the threats to internal validity are those caused by the characteristics of the experimental process and its setting. In the following, we describe some of the most common internal threats reported in the literature. In particular, we focus on those that could be automatically warned, detected or fixed. For a better understanding, each threat is presented by means of: i) a brief definition, ii) examples of situations where the threat could appear in experiments #1 and #2, iii) possible mechanisms to diagnose (i.e., detect) the threat, and iv) ways of neutralizing the threat. The internal validity threats can be classified into four groups, describied in the following subsections. </p>				<h5>Threats caused by environmental factors and nuisance variables </h5>								<h6>					<a role="button" data-toggle="collapse" href="#ivt1" aria-expanded="false" aria-controls="ivt1">						IVT-1 Wrong temporal precedence					</a>				</h6>				<div class="collapse" id="ivt1">					<p><i>Definition:</i> The measurements are erroneously taken before applying the treatment. </p>					<p><i>Example:</i> In experiment #1, a nurse could measure the corporal temperature of a patient before administering the drug. In experiment #2, an implementation bug could make the program to return the initial solution found as a result, instead of the best solution found during all iterations. </p>					<p><i>Diagnosis:</i> This threat is difficult to diagnose in general. However, in computational experiments, the execution environment can monitor the conduction of the experiment ensuring that no measures are taken before the treatments. </p>					<p><i>Neutralization:</i> This threat is neutralized fixing the problems that cause a wrong sequence of treatment-measurement and repeating the conduction of the experiment. This cannot be done automatically in general. </p>				</div>				<h6>					<a role="button" data-toggle="collapse" href="#ivt2" aria-expanded="false" aria-controls="ivt2">						ITV-2 History and environmental factors					</a>				</h6>					<div class="collapse" id="ivt2">						<p><i>Definition:</i> External events influence the conduction of the experiment affecting the outcome. </p>						<p><i>Example:</i> In experiment #1 could happen simultaneously to a heat wave falsifying the effect of the drug. In experiment #2, an operating system update could happen simultaneously to the execution of a program, decreasing the computational resources available for its execution. </p>						<p><i>Diagnosis:</i> Since the events that cause the bias on the outcome are not predictable nor monitored in general, there is not a general mechanism for diagnosing this threat. </p>						<p><i>Neutralization:</i> According to Shadish et al. and Gliner et al. the best approach to minimize this threat is the use of a random assignment and a randomized sequence of treatment application. </p>					</div>					<h6>							<a role="button" data-toggle="collapse" href="#ivt3" aria-expanded="false" aria-controls="ivt3">							ITV-3 Testing effects						</a>					</h6>					<div class="collapse" id="ivt3">						<p><i>Definition:</i> Treatments on an experimental object affect other treatments. </p>						<p><i>Examples:</i> In experiment #1, the reaction to the drug could be different in patients that had already taken the drug before. In experiment #2 the effect of memory caches and predictive executions in modern processors could lead to an improvement of the results provided by some techniques. </p>						<p><i>Diagnosis:</i> To be the best of our knowledge, there is no methods to diagnose this threat. A tentative approach would be to compute the correlation of the values of the outcome with the index of the treatments related to the measurements in the sequence of the experimental protocol (globally, per block, and per group). If there is an strong correlation between the values, then the conclusions of the experiment could be threatened. </p>						<p><i>Neutralization:</i> Again, the recommended approach to minimize this threat is the use of a random assignment and a randomized sequence of treatment application. </p>					</div>					<h6>							<a role="button" data-toggle="collapse" href="#ivt4" aria-expanded="false" aria-controls="ivt4">							ITV-4 Instrumentation effects						</a>					</h6>					<div class="collapse" id="ivt4">						<p><i>Definition:</i> The way in which a variable is measured has an effect on its value. </p>						<p><i>Example:</i> In experiment #1, the conclusions would be threatened if the instrument used for measuring body temperature could get deteriorated during the experiment providing different measurements. In experiment #2, this threat could be a risk if the solutions generated by the metaheuristic program were dynamic, changing along time. </p>						<p><i>Diagnosis:</i> This threat can be diagnosed by obtaining several measurements prior and after the conduction of the experiment. Another diagnosis strategy for this threat is using multiple instrumentation artefacts during the experimental conduction. </p>						<p><i>Neutralization:</i> To the best of our knowledge the only way to neutralize this threat is to use different artefacts for the measurement. This strategy cannot be automated in general. The use of multiple instrumentation artefacts and its random assignment to perform the measurements during experimental conduction can also mitigate the effects of this threat. </p>					</div>					<h5>Threats caused by the characteristics of groups </h5>						<h6>								<a role="button" data-toggle="collapse" href="#ivt5" aria-expanded="false" aria-controls="ivt5">							ITV-5 Heterogeneity of experimental objects						</a>					</h6>					<div class="collapse" id="ivt5">						<p><i>Definition:</i> Experimental objects are not homogeneous. As a result, the effects of the treatments on the outcomes are confounded with that of the specific levels of the noncontrollable factors of the experimental objects. </p>						<p><i>Examples:</i> In experiment #1, if babies and adults are mixed in the groups the effect of the drug could be confounded due to the difference in the relation between weight and dose; i.e., in babies weighting 10 kilograms a dose of 100mg can cause a much bigger effect than in adults weighting 100 kilograms. </p>						<p><i>Diagnosis:</i> This threat arises if the experimental description contains non-controllable factors that are not used in the blocking of the design. This diagnosis should be interpreted as a warning for the experimenters. Another approach would be to measure the correlation between the levels of non-controllable factors not present in the blocking criterion and the outcome. If there is a strong correlation between the variables the conclusions drawn are threatened. </p>						<p><i>Neutralization:</i> The only way to minimize this threat is to reconduct the experiment introducing the non-controllable factors as blocking criteria. </p>					</div>					<h6> 								<a role="button" data-toggle="collapse" href="#ivt6" aria-expanded="false" aria-controls="ivt5">							ITV-6 Attrition						</a>					</h6>					<div class="collapse" id="ivt6">						<p><i>Definition:</i> The measurement of outcomes for some experimental object fail, are lost, or become impossible to obtain during experimental conduction. This threat is also referred to as mortality. </p>						<p><i>Examples:</i> In experiment #1 some patients could die during the conduction of the experiment, leading to no measurements of the outcome. It is worth noting, that certain levels of attrition are acceptable (but not desirable) in experimental areas such as biology, medicine, etc. </p>						<p><i>Diagnosis: </i>Based on the experimental description the expected number of outcome measurements can be computed for classical designs. Since the measurements of the outcome variable should be provided as part of the lab-pack, it is possible to diagnose this threat by comparing the expected and actual number of measurements. </p>						<p><i>Neutralization:</i> The only way to neutralize this threat is repeat the conduction of the experiment. For MOEs this mechanism can be automated. </p>					</div>					<h5>Threats related to the statistical analysis of the data </h5>					<h6> 								<a role="button" data-toggle="collapse" href="#ivt7" aria-expanded="false" aria-controls="ivt7">							ITV-7 Small sampling  						</a>					</h6>					<div class="collapse" id="ivt7">						<p><i>Definition:</i> Statistical tests do not recognize as statistically significant the differences in the observations when the sample is very small. This threat is also referred to as low statistical power. </p>						<p><i>Example:</i> In experiment #1 if the number of feverish patients in the Hospital is small, for instance 5, the experiment conclusions would be threatened since specific and rare characteristics of one or two of the patients could lead to wrong conclusions. </p>						<p><i>Diagnosis:</i> A simple diagnostic mechanism for this threat is to check if the size of the sample is larger than a minimum. Historically, authors have used 30 experimental objects as such minimum for simple comparisons. </p>						<p><i>Neutralization:</i> There are several ways to increase the statistical power, but the most usual is to increase the size of the sampling. In MOEs, this can be usually automated by increasing the number of run of the metaheuristic programs. </p>					</div>					<h6> 								<a role="button" data-toggle="collapse" href="#ivt8" aria-expanded="false" aria-controls="ivt8">							IVT-8 Violations of the preconditions of statistical test 						</a>					</h6>					<div class="collapse" id="ivt8">						<p><i>Definition:</i> The preconditions of the statistical tests are not met and them conclusions drawn from them are erroneous. </p>						<p><i>Example: </i> In experiment #2, depending on the distribution and characteristics of the data, the recommended statistical test for the experiment according to table 3 would be either a T-test or a Wilcoxon test. If the data is not normal and the T-test is applied, the preconditions of the T-test would be violated and the conclusions could be erroneous. </p>						<p><i>Diagnosis:</i> Most of the preconditions of statistical tests, such as normality or homoscedasticity of data, can be evaluated through other statistical tests. Thus, those auxiliary tests can be used to check whether certain preconditions are fulfilled or not. </p>						<p><i>Neutralization: </i>In order to neutralize this threat, the statistical analysis must be repeated using statistical tests whose preconditions are not violated. The automation of this mechanism involves selecting and running the statistical test automatically. </p>					</div>					<h6> 								<a role="button" data-toggle="collapse" href="#ivt9" aria-expanded="false" aria-controls="ivt9">							IVT-9 Fishing and error rate						</a>					</h6>					<div class="collapse" id="ivt9">						<p><i>Definition:</i> Several comparisons among pairs of observations are performed using simple comparison tests. As a result the error rate is accumulated and the conclusions of statistical tests are misleading. </p>						<p><i>Example: </i>In experiment #2 up to five optimization techniques are compared. If we apply a simple comparison tests, such as the T-test with a = 0.05 to evaluate the significance of the differences for each couple of techniques, we would perform 10 simple comparisons. The probability of error of the whole set of comparisons is not 0.05, but significantly higher . Thus, a multiple comparison tests with post-hoc procedures should be used. 						<p><i>Diagnosis:</i> The automated diagnosis of this threat requires the analysis of the experimental description to determine the number of comparisons to be performed and the type of the test specified. </p>						<p><i>Neutralization: </i>In order to neutralize this threat, the statistical analysis must be repeated using appropriate multiple comparison statistical tests and post-hoc procedures. Again, the automation of this mechanism involves the automated selection and execution of the statistical test. </p>					</div>					<h6>						<a role="button" data-toggle="collapse" href="#ivt10" aria-expanded="false" aria-controls="ivt10">							IVT-10 Restriction of range 						</a>					</h6>					<div class="collapse" id="ivt10">						<p>Definition: Variables are restricted to a narrow range that do not match the actual domain of the observations. </p>						<p><i>Example: </i>This threat is equivalent to the out of range errors and precision losses due to castings in programming languages. For instance, in experiment #1 we could use [36,40] as the valid range for the patient temperature. If we get a measurement of 41, it would be truncated during the analysis of the data leading to erroneous conclusions. In experiment #2, if the objective function is real with a range between [0,500.000], but the levels specified for the experimental variable are in the range [0,10] (meaning that measurements of values bigger than 10 are interpreted as a level of 10), it is probable that almost any observation were associated with the level 10. Consequently, the statistical test could miss actual differences in the distributions of observations of the variable. </p>						<p><i>Diagnosis:</i> This threat can be diagnosed chekcing that all the values registered at the end of the experiment are in the ranges defined for each variable. We are not aware of any diagnosis method for the cases in which the values are in the range but that range does not match the actual range of the observations. </p>						<p><i>Neutralization:</i> The experiment should be repeated using an an adequate range for the variables. </p>					</div>					<h6>						<a role="button" data-toggle="collapse" href="#ivt11" aria-expanded="false" aria-controls="ivt11">							IVT-11 Inaccurate size estimation effect						</a>					</h6>					<div class="collapse" id="ivt11">						<p><i>Definition:</i> Some statistical tests overestimate or underestimate the differences in the observations depending on the type of variable. This leads experimenters to draw wrong conclusions about the experimental hypothesis. 						<p><i>Example: </i>In experiment #2, we could use a Quade's test for multiple comparison. However, it is well known  that this test overestimates the differences of results between techniques when problem instances are very different. 						<p><i>Diagnosis:</i> The diagnosis of this threat depends on behaviour of the specific statistical test or post-hoc procedure and on the specific experiment. For instance, the use of Quade's test could be admissible if the difference of behaviour for the techniques with different problem instances would be of paramount importance for solving the optimization problem. However its use is not recommended in general. As a consequence, the diagnosis of this threat should be interpreted as a warning for experimenters, that should be confirmed. 						<p><i>Neutralization:</i> In order to neutralize this threat, the statistical analysis must be repeated using appropriate multiple comparison statistical tests and post-hoc procedures. </p>					</div>					<h5>Threats caused by the the characteristics of the experimental conduction </h5>					<h6>						<a role="button" data-toggle="collapse" href="#ivt12" aria-expanded="false" aria-controls="ivt12">							IVT-12 Unreliability of treatment implementation  	    						</a>					</h6>					<div class="collapse" id="ivt12">						<p><i>Definition:</i> The implementation of the treatment on experimental object is erroneous. </p>						<p><i>Example: </i>In experiment #1 some nurses, could forget administrating the drug to several patients, or could administer a different dose. </p>						<p><i>Diagnosis: </i>This could be diagnosed by analysing the variance and the outliers of the distributions of measurement per instrumentation artefact, block and group. However, this method is applicable when groups have a minimum size, and the variance of the distribution is small.</p>						<p><i>Neutralization:</i> If unreliable measures are detected as outliers, the filtering of those measurements is a valid neutralization mechanism. Otherwise, removing the threat could require the conduction of the experiment with different instrumentation artefacts. </p>					</div>				</div>				<h4>					<a role="button" data-toggle="collapse" href="#externalValidity" aria-expanded="false" aria-controls="externalValidity">						5.2 External Validity  	    					</a>				</h4>				<div class="collapse" id="externalValidity">					<p>The external validity of an experiment is defined as the extent to which conclusions can be generalized. Thus, most external validity issues are related to experimental objects, settings, treatments or outcomes that were not studied in the experiment. </p>					<h5> 						<a role="button" data-toggle="collapse" href="#threatsToExtVal" aria-expanded="false" aria-controls="threatsToExtVal">							Threats to external validity 	    						</a>					</h5>					<div class="collapse" id="threatsToExtVal">						<p>The threats to the <i>external validity</i> of the experiment are those that could affect the way in which the conclusions are generalized from the experimental sample to the target population. Next, we enumerate those proposed. The information regarding the diangosis and neutralization of external validity threats are omitted since it is out of the scope of this work. </p>						<p><b>Interaction of experimental objects with factor effects:</b> A conclusion drawn with a sample could not be extrapolated to a different sample. In experiment #1, the reductions of body temperature observed with specific feverish patients from Sevilla could be different form those observed on a different set of patients. </p>						<p><b>Interaction of treatment implementation and factor effects:</b> The effect observed for a treatment could vary depending on the specific details of their application. In experiment #1, the effect of the drug could be different if patients gulp it with water or not. </p>						<p><b>Interaction of factor effects and outcome variable:</b> The effect observed for a treatment could depend on the specific outcome variable measured. In experiment #2, the effect of the drug could be different if we measure fever through the amount of sweat. </p>						<p><b>Interaction of factor effects and experimental setting:</b> The effect observed for a treatment could depend strongly on specific elements of the experimental setting or the context of the experiment. In experiment #1, the reduction of body temperature could depend on the temperature of the room where the treatments were applied. </p>					</div>				</div>				</div>			<h3>				<a role="button" data-toggle="collapse" href="#moe" aria-expanded="false" aria-controls="moe">					6 METAHEURISTIC OPTIMIZATION EXPERIMENTS     				</a>			</h3>			<div id="moe" class="collapse">				<p>Metaheuristic optimization experiments have some characteristics that make them even more time consuming and difficult to set-up than other computational algorithmic experiments. First, the stochastic nature of the algorithms makes it necessary a high number of runs per group to get significant results, leading to long-running experiments. Second, even using a MOF a significant development effort is needed. This development effort is due to the need of adapting the techniques to the problem at hand (by using the extension points of the MOFs that encode the tailoring points of the implemented metaheuristics), and implement the experimental procedure, thus we will have bugs that are usually detected during the execution of the experiment or in the data analysis of the results. As a consequence, experiments are usually run several times until reaching to valid results. Third, the high number of variants and parameters that the different techniques present lead to experimental variables with a high number of levels and complex designs. Finally, one experiment usually requires the realization of subordinate experiments, magnifying the effect of the previous characteristics. For instance, in technique comparison experiments the performance of the techniques depends strongly on the parameter values used. As a consequence, in order to perform a fair comparison, one subordinate experiment per technique must be performed, in order to find its optimal parameter configuration . Next we describe the types of MOEs taken into account in this dissertation to support the MPS life-cycle. </p>					<h4>						<a role="button" data-toggle="collapse" href="#selTalo" aria-expanded="false" aria-controls="selTalo">							6.1 Selection and Tailoring experiments  	    						</a>					</h4>					<div class="collapse" id="selTalo">						<p>Given an optimization problem <i>P =(f , A)</i>and a set of metaheuristic algorithms <i>M ={M<sub>1</sub>,. . ., M<sub>n</sub>}</i>, the goal of this experiment in to determine the techniques that perform better than all the others; i.e., find the best techniques to solve <i>P</i>. Each metaheuristic algorithm is run with a set of parameter values that is constant along the experiment. Each metahuristic <i>M<sub>i</sub></i> generates a set of solutions <i>S<sub>i</sub> ={s<sub>i,1</sub>i,. . ., s<sub>i,rruns</sub>}</i>, where <i>s<sub>i,j</sub> &isin; A</i> is the solution generated by the <i>j</i> -th run of <i>M<sub>i</sub></i>.</p> 												<p>This notion of better performance usually operationalized as the maximum average value on <i>f</i> of the solutions provided by each technique in maximization problems, i.e. <i>avg(f,S<sub>i</sub>)=&sum;<sup>|S<sub>j</sub>|</sup><sub>j=1</sub>f(s<sub>i,j</sub>)/|S<sub>i</sub>|</i>. Thus the goal of the experiment is finding the subset <i>M<sup>*</sup>&sube; M</i> such that <i>&forall;(M<sub>i</sub>&isin; M<sup>*</sup> ,M<sub>k</sub> &notin; M<sup>*</sup>)&#8729; avg(f,S<sub>i</sub>) > avg(f,S<sub>k</sub>)</i>.</p>						<p>The experimental object in this experiment are runs of metaheuristics in <i>M</i> solving <i>P</i>. There is one single controllable factor (we name it "technique"), whose levels are labels corresponding to the different metaheuristics <i>{'M'<sub>i</sub>,...,'M'<sub>n</sub>}</i>. The outcome is the nvalue on <i>f</i> of the solution provided by each experimental object (algorithm run). </p>						<p>The hypothesis of this type of experiment is differential, stating that the specific metaheuristic used to solve the problem has an impact on the value on f of the solutions provided. In statistical terms, the null hypothesis <i>H<sub>0</sub><sup>TC</sup></i> of this type of MOE states 0 that "there is no difference in the mean value on f for the populations of solutions generated by the metaheuristics". The alternative hypothesis <i>H<sub>1</sub><sup>TC</sup></i> states that "there is 1 a significant difference in the mean value on f for the populations of solutions generated by the metaheuristics", i.e., that there are some techniques that perform better than the rest. </p>						<p>Usually, this comparison is performed not for a single problem instance, but for a finite set <i>I ={I<sub>1</sub>,. . ., I<sub>m</sub>}</i> of problem instances. Since the specific problem instance to be solved can have an important impact on the outcome of the experiment, it is treated usually as a blocking variable playing the role of nuisance factor. </p>						<p>The design of this type of MOE is usually a blocking factorial, where each metaheuristic <i>M<sub>i</sub></i> is executed <i>nruns</i> times on each problem instance <i>I<sub>k</sub></i>. The analysis for testing the statistical hypothesis depends on the number of metaheuristics in M. Since one single factor is present, the techniques described in table 3 are used. </p>						<p>When we compare a pair of metaheuristics (<i>|M| =2</i>), then we have a simple comparison experiment, and the recommended analyses are the T-Test and the Mann-Withney or Wilcoxon's Tests if some of the premises of the T-test are violated (typically normality, but also sphericity or homoscedasticy, and independence). If the statistical test rejects the null hypothesis we conclude that some metaheuristics are better than others, and use the means computed to determine which the best is. </p>						<p>When comparing three or more metaheuristics, then we have a multiple comparison experiment. The recommended analyses are ANOVA and Friedman's test if some of the premises of the ANOVA are violated (typically normality, sphericity or homoscedasticy, and independence). If the statistical test rejects the null hypothesis we conclude that some metaheuristics are better than others. In this case we need to perform post-hoc tests to determine among which metaheuristic are significant differences, and determine those that perform better. </p>						<p>A slight variant of this kind of MOE is when several tailorings (algorithms) generated for the same metaheuristic are compared. The hypotheses, experimental objects, design and analyses are similar to those used for selection experiments, hence the generalization as technique comparison experiments. </p>					</div>					<h4>						<a role="button" data-toggle="collapse" href="#tunningExp" aria-expanded="false" aria-controls="tunningExp">							6.2 Tuning Experiments 	    						</a>					</h4>					<div class="collapse" id="tunningExp">						<p>In this type of MOE a single metaheuristic algorithm <i>M<sub>x</sub> </i> having a set of parameters <i>{&rho;<sub>i</sub>,. . ., &rho;<sub>n</sub>}</i> with specific domains <i>{D<sub>i</sub>,. . ., D<sub>n</sub>}</i> is used. The goal of the experiment is to find an optimal parametrization of <i>M<sub>x</sub></i>, i.e., the parameter values <i>V<sup>*</sup> =(&upsilon;<sup>*</sup><sub>1</sub> ,..., &upsilon;<sup>*</sup><sub>n</sub>) </i>that provide the best performance for solving <i>P</i> with <i>M<sub>x</sub></i> where <i>&upsilon;<sub>i</sub> &isin; D<sub>i</sub>,i =1, . . . , n</i>. The experimental object in this experiment are runs of <i>M<sub>x</sub></i> solving <i>P</i>. The otucome is the value on f of the solution provided by each experimental object (algorithm run). There is one controllable factor per parameter, and the levels of such variables depend on the specific domain of each parameter. The comparison is usually performed not for a single problem instance, but for a finite set <i>I ={I<sub>1</sub>,. . ., I<sub>m</sub>}</i> of problem instances. Since the specific problem instance to be solved can have an important impact on the outcome of the experiment, it is treated usually as a blocking variable playing the role of nuisance factor. </p>						<p>The main difference between both types of experiments is that the number of factors and their levels can be much higher for Technique Tuning Experiments than for Technique Comparison Experiments. Furthermore, the domain of some parameters in Technique Comparison Experiments can have an infinite number of values, leading to its discretization by the experimenter, to the use of complex designs, or to carrying out a number related Technique Tuning Experiments for performing the Tuning stage of the MPS. </p>						<p>For the purpose of this dissertation, a simple blocking factorial design is chosen for this kind of experiments. Problem instance is usually treated as a blocking variable due to the specific problem instance may strongly impact on the outcome. However, the contributions of this dissertation are extensible and in most cases can be adapted to alternative designs. In the remainder of this subsection the use of this design is assumed. </p>						<p>The null hypothesis of this type of MOE states that "there is no difference in the mean value on <i>f</i> for the populations of solutions generated by the any of the parametrizations chosen". The alternative hypothesis states that "there is a significant difference in the mean value on <i>f</i>  for the populations of solutions generated by the metaheuristics", i.e., there are some parametrizations that perform better than the rest. </p>						<p>The analysis for testing the hypotheses is similar to that specified for Technique Comparison Experiments when using a blocking factorial design, but in this cases multiple factors are present, leading to the use of the techniques described in table 4. </p>					</div>					<h4>						<a role="button" data-toggle="collapse" href="#moedesign" aria-expanded="false" aria-controls="moedesign">							6.3 Designs for MOEs 	    						</a>					</h4>					<div class="collapse" id="moedesign">						<p>The experimental designs used for selection and tailoring experiments are usually classical designs. When the experiment has a single controllable factor, complete randomized designs are used (with blocking when non-controllable factors are present), since the problem instance is usually a blocking factor the most usual design in this case is randomized complete block design. In a very similar way, when the experiment has multiple controllable factors (such as in tailoring experiments with variants in multiple tailoring points), factorial and factorial blocking designs are applied. </p>						<p>Tuning experiments present a much wider diversity of experimental design. Although, factorial and factorial blocking designs are usually applied, several of complex designs for this kind of MOE has been proposed in literature. The most relevant proposals in this area are: Response surface designs, SPO (Sequential Parameter Optimization), and racing algorithms. It is worth noting that most of those proposals do not provide an explicit experimental protocol, but algorithm that decides the experimental protocol at condution time based on the observations obtained at each moment. </p>					</div>					<h4>						<a role="button" data-toggle="collapse" href="#moeanalyses" aria-expanded="false" aria-controls="moeanalysis">							6.4 Analyses for MOEs	    						</a>					</h4>					<div class="collapse" id="moeanalyses">						<p>MOEs present some specific characteristics that had led to the development of specific way for deciding which specific statistical analysis techniques should be used. In the specific context of computer science, the assumptions enumerated above are less likely than in natural or social sciences such as biology and psychology where variables are usually normal. In this sense, MOEs require a wide spectrum of tests of hypothesis. </p>						<p>Furthermore, multiple comparison tests have one important drawback: they can only detect significant differences over the whole set of data; i.e., the test detects that there are significant differences between any of the multiple compared groups, treatments or algorithms, but it does not identify between which ones. One could think of applying simple comparison tests to detect the differences between every pair of variables in the dataset but this process introduces an important error risk. This error comes from the combination of many pairwise comparisons, that involves increasing the probability of rejecting one or more true null hypotheses (in statistical terminology, this error is denoted as the Family Wise Error Rate). In these circumstances the use of post-hoc procedures is recommended. Useful practical considerations for nonparametric multiple comparisons test are provided in including recommendations for the selection of the post-hoc procedures to be applied.</p>					</div>					<h4> 						<a role="button" data-toggle="collapse" href="#moethreats" aria-expanded="false" aria-controls="moethreats">							6.5 Threats to validity in MOEs	    						</a>					</h4>					<div class="collapse" id="moethreats">						<p>Experimental conduction in MOEs implies the repeated execution of different metaheuristic programs to find solution on each problem instances. The assignment of the executions to metaheuristic programs and problem instances is usually randomized and automated, thus avoiding human bias. Furthermore, since experimental objects are metaheuristic executions on specific problem instances, no risk of mortality or attrition exists. <p>						<p>However, even with such an automated experiment conduction and assignment, there exists risk of maturation, testing or memory effects. For instance, the effect of memory caches and predictive executions in modern processors could lead to an improvement of the results provided by the programs if their execution is repeated sequentially. In order to minimize the impact of such effects on the results of the experiment, the order in which metaheuristic programs are run on the different problem instances is randomized. MOEs are also affected by all the threats regarding to the validity of the analysis. For instance, fishing and error rate threats are also important, hence the need of using multiple comparison tests and posthoc procedures. </p>						<p>Additionally, <i>history threats</i>  (also called <i> extraneous environmental events</i> ) can also affect MOEs. This threat is caused by the concurrency of events that affect the experimental environment with the experiment conduction, impacting on the set of out-comes of the experiment. For instance, an automated operating system update could be performed on the experimentation computer while the experiment is carried out. In this situation, the measured performance of the techniques could be affected, depending on the specific stop criteria used in the experiment. </p>						<p>Finally, since metaheuristic algorithms must be implemented as programs MOEs are threatened unreliability of treatment implementation (implementations can have bugs). </p>					</div>				</div>			</div></body>	</html>